{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple corpus for example\n",
    "corpus = [\"I like going to the park\", \"I like playing football\", \"The park is near my house\", \n",
    "          \"Do you like going to the park?\", \"Playing at the park is fun\", \"Football is a popular sport\"]\n",
    "\n",
    "# Create a vocabulary dictionary\n",
    "vocab = set()\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    vocab.update(words)\n",
    "    \n",
    "print(vocab)\n",
    "\n",
    "word2id = {word: i for i, word in enumerate(vocab)}\n",
    "id2word = {i: word for word, i in word2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurrence Matrix X\n",
    "X = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words):\n",
    "        for j in range(max(i - 1, 0), min(i + 2, len(words))):\n",
    "            if i != j:\n",
    "                X[word2id[word]][word2id[words[j]]] += 1\n",
    "\n",
    "# Non-zero co-occurrences\n",
    "non_zero_coocs = np.transpose(np.nonzero(X))\n",
    "\n",
    "# Hyperparameters\n",
    "vector_size = 20\n",
    "iterations = 200\n",
    "learning_rate = 0.05\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "\n",
    "# Weight function\n",
    "def f(x):\n",
    "    if x < x_max:\n",
    "        return (x / x_max) ** alpha\n",
    "    return 1\n",
    "\n",
    "# Initialization\n",
    "W = np.random.rand(len(vocab), vector_size) # center word vectors\n",
    "W_tilde = np.random.rand(len(vocab), vector_size) # context word vectors\n",
    "b = np.random.rand(len(vocab)) # bias for center words\n",
    "b_tilde = np.random.rand(len(vocab)) # bias for context words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(iterations):\n",
    "    total_loss = 0\n",
    "    for i, j in non_zero_coocs:\n",
    "        weight = f(X[i, j])\n",
    "        inner_cost = W[i, :].dot(W_tilde[j, :]) + b[i] + b_tilde[j] - np.log(X[i, j])\n",
    "        cost = weight * (inner_cost ** 2)\n",
    "        total_loss += 0.5 * cost\n",
    "\n",
    "        # Gradients\n",
    "        grad_cost = weight * inner_cost\n",
    "        grad_wi = grad_cost * W_tilde[j, :] \n",
    "        grad_wj = grad_cost * W[i, :]\n",
    "        grad_bi = grad_cost\n",
    "        grad_bj = grad_cost\n",
    "\n",
    "        # Update vectors and biases\n",
    "        W[i, :] -= learning_rate * grad_wi\n",
    "        W_tilde[j, :] -= learning_rate * grad_wj\n",
    "        b[i] -= learning_rate * grad_bi\n",
    "        b_tilde[j] -= learning_rate * grad_bj\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize vectors\n",
    "W = normalize(W)\n",
    "for i in range(len(vocab)):\n",
    "    print(f\"{id2word[i]}: {W[i, :]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
