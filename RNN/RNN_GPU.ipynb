{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import reuters\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tb46XhZpGwW",
        "outputId": "3b4677ea-320e-40f0-f18c-10ef37f38b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import reuters\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preparing the data\n",
        "sentences = reuters.sents()\n",
        "sentences = [[\"<BOS>\"] + [word.lower() for word in sentence] + [\"<EOS>\"] for sentence in sentences]\n",
        "\n",
        "# Prepare the vocabulary\n",
        "PAD_TOKEN = \"_PAD\"\n",
        "UNK_TOKEN = \"_UNK\"\n",
        "word2idx = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word] = len(word2idx)\n",
        "vocab = word2idx\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "idx2word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# Prepare the datasets\n",
        "data = [[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence[:-1]] for sentence in sentences]\n",
        "targets = [[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence[1:]] for sentence in sentences]\n",
        "\n",
        "data_train, data_test, targets_train, targets_test = train_test_split(data, targets, test_size=0.2)\n",
        "\n",
        "# Define Dataset\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences, targets):\n",
        "        self.sentences = sentences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sentences[idx]), torch.tensor(self.targets[idx])\n",
        "\n",
        "# Prepare data loaders\n",
        "def pad_collate(batch):\n",
        "    (xx, yy) = zip(*batch)\n",
        "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=vocab[PAD_TOKEN])\n",
        "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=vocab[PAD_TOKEN])\n",
        "    return xx_pad, yy_pad\n",
        "\n",
        "train_data = SentenceDataset(data_train, targets_train)\n",
        "test_data = SentenceDataset(data_test, targets_test)\n",
        "batch_size = 24\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=pad_collate)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=pad_collate)\n",
        "\n",
        "# Define the model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[PAD_TOKEN])\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, _ = self.rnn(x)\n",
        "        x = self.fc(output)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "hidden_size = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = RNNModel(vocab_size, embedding_dim, hidden_size).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training\n",
        "for epoch in range(5):  # For simplicity, we just use 5 epochs\n",
        "    model.train()\n",
        "    for data, targets in tqdm(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output.view(-1, vocab_size)\n",
        "        targets = targets.view(-1)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Save the model weights\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLpJ9e5xp8MM",
        "outputId": "46dd84ad-61ac-46dc-a55d-ac6c0a3a9efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "100%|██████████| 1824/1824 [00:42<00:00, 43.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 4.56048059463501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1824/1824 [00:41<00:00, 43.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Loss: 4.333864688873291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1824/1824 [00:41<00:00, 43.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Loss: 4.291331768035889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1824/1824 [00:42<00:00, 43.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Loss: 4.3650803565979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1824/1824 [00:41<00:00, 43.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Loss: 4.353855133056641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "sentence = \"<BOS> You\".split()\n",
        "x = torch.tensor([[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence]], device=device)\n",
        "output = model(x)\n",
        "prediction = output[0, -1].argmax(dim=0).item()  # Get the predicted next word for the last word in the sentence\n",
        "predicted_word = idx2word.get(prediction, UNK_TOKEN)\n",
        "print(f\"Predicted next word: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPfvOoMLnj0t",
        "outputId": "290cd944-cec1-42aa-fec1-c22df3e711a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word: ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "length = 30\n",
        "sentence = \"<BOS> What was\".split()\n",
        "for _ in range(length):  # Generate 10 words\n",
        "    x = torch.tensor([[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence]], device=device)\n",
        "    output = model(x)\n",
        "    prediction = output[0, -1].argmax(dim=0).item()  # Get the predicted next word for the last word in the sentence\n",
        "    predicted_word = idx2word.get(prediction, UNK_TOKEN)\n",
        "    if predicted_word != \"<EOS>\":\n",
        "        sentence.append(predicted_word)  # Add the predicted word to the sentence if it's not \"<EOS>\"\n",
        "\n",
        "print(f\"Generated sentence: {' '.join(sentence)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPGQcc4hwSww",
        "outputId": "2d10c759-9274-4236-9654-b9a9377be6dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence: <BOS> What was expected to be a little comment on the agreement , which is expected to be taken .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pcJ5zMSbwVE3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}