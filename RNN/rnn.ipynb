{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "sentences = reuters.sents()\n",
    "sentences = [[\"<BOS>\"] + [word.lower() for word in sentence] + [\"<EOS>\"] for sentence in sentences]\n",
    "# print(sentences[1]) ['<BOS>', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u', '.', 's', '.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u', '.', 's', '.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.', '<EOS>']\n",
    "# print(sentences[2]) ['<BOS>', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', '-', 'run', ',', 'in', 'the', 'short', '-', 'term', 'tokyo', \"'\", 's', 'loss', 'might', 'be', 'their', 'gain', '.', '<EOS>']\n",
    "# print(sentences[3]) ['<BOS>', 'the', 'u', '.', 's', '.', 'has', 'said', 'it', 'will', 'impose', '300', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', '17', ',', 'in', 'retaliation', 'for', 'japan', \"'\", 's', 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', '.', '<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the vocabulary based on the whole text\n",
    "PAD_TOKEN = \"_PAD\"\n",
    "UNK_TOKEN = \"_UNK\"\n",
    "word2idx = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "for sentence in sentences:\n",
    "    for word in sentence:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "vocab = word2idx\n",
    "vocab_size = len(vocab)\n",
    "idx2word = {idx: word for word, idx in vocab.items()}\n",
    "# count = 0\n",
    "# for key, value in idx2word.items():\n",
    "#     print(key, value)\n",
    "#     count += 1\n",
    "#     # Break the loop after printing the first 10 elements\n",
    "#     if count == 20:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets\n",
    "data = [[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence[:-1]] for sentence in sentences]\n",
    "targets = [[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence[1:]] for sentence in sentences]\n",
    "\n",
    "data_train, data_test, targets_train, targets_test = train_test_split(data, targets, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences, targets):\n",
    "        self.sentences = sentences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.targets[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=vocab[PAD_TOKEN])\n",
    "    yy_pad = pad_sequence(yy, batch_first=True, padding_value=vocab[PAD_TOKEN])\n",
    "    return xx_pad, yy_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SentenceDataset(data_train, targets_train)\n",
    "test_data = SentenceDataset(data_test, targets_test)\n",
    "batch_size = 24\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=pad_collate)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[PAD_TOKEN])\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        x = self.fc(output)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 64\n",
    "hidden_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model ###\n",
    "You can skip this part and move for the other cell if you want to use the already trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(5):  # For simplicity, we just use 5 epochs\n",
    "    model.train()\n",
    "    for data, targets in tqdm(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.view(-1, vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    \n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('model_weights.pth', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next word: ,\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "sentence = \"<BOS> You\".split()\n",
    "x = torch.tensor([[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence]], device=device)\n",
    "output = model(x)\n",
    "prediction = output[0, -1].argmax(dim=0).item()  # Get the predicted next word for the last word in the sentence\n",
    "predicted_word = idx2word.get(prediction, UNK_TOKEN)\n",
    "print(f\"Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence: <BOS> I thought about it a lot and I got the right decision regarding to the u . s . , which has been expected to be reached by the market , which bodes , dashing by the u . s . the u\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "length = 30\n",
    "sentence = \"<BOS> I thought about it a lot and I got the right decision regarding to the\".split()\n",
    "generated_sentence = sentence.copy()  # This will be used to print the generated sentence\n",
    "for _ in range(length):  # Generate 30 words\n",
    "    x = torch.tensor([[vocab.get(word, vocab[UNK_TOKEN]) for word in sentence]], device=device)\n",
    "    output = model(x)\n",
    "    prediction = output[0, -1].argmax(dim=0).item()  # Get the predicted next word for the last word in the sentence\n",
    "    predicted_word = idx2word.get(prediction, UNK_TOKEN)\n",
    "    sentence.append(predicted_word)  # Always add the predicted word to the sentence\n",
    "    if predicted_word != \"<EOS>\":\n",
    "        generated_sentence.append(predicted_word)  # Only add the predicted word to the generated sentence if it's not \"<EOS>\"\n",
    "\n",
    "print(f\"Generated sentence: {' '.join(generated_sentence)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
