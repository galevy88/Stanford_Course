{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER ###\n",
    "Notice that this is vanilla example and the train set sentences are composed of very narrow vocabulary. Therefore the predictions for the unknown words during the train phase may be not good. But notice that for the known words this is a good model.</br>\n",
    "If you want you can upload your own dataset and train the model based on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(n):\n",
    "    person_prefixes = [\n",
    "    ['I', 'meet'], ['She', 'knows'], ['He', 'saw'], ['We', 'spoke'], ['They', 'contact'], \n",
    "    ['They', 'met'], ['I', 'interviewed'], ['She', 'recognized'], ['He', 'noticed'], ['We', 'greeted'],\n",
    "    ['They', 'welcomed'], ['I', 'found'], ['She', 'introduced'], ['He', 'mentioned'], ['We', 'saw'],\n",
    "    ['They', 'had dinner with'], ['I', 'chatted with'], ['She', 'appreciated'], ['He', 'hugged'], ['We', 'appreciated'],\n",
    "    ['They', 'talked to'], ['I', 'praised'], ['She', 'helped'], ['He', 'listened to'], ['We', 'thanked'],\n",
    "    ['They', 'informed'], ['I', 'advised'], ['She', 'invited'], ['He', 'accepted'], ['We', 'met with']\n",
    "    ]\n",
    "\n",
    "    location_prefixes = [\n",
    "    ['I', 'visit'], ['We', 'are in'], ['They', 'leave'], ['She', 'is from'], ['He', 'lives'],\n",
    "    ['I', 'arrived'], ['We', 'travel'], ['They', 'explore'], ['She', 'moved'], ['He', 'commutes'],\n",
    "    ['I', 'love'], ['We', 'vacationed in'], ['They', 'study in'], ['She', 'works in'], ['He', 'going'],\n",
    "    ['I', 'recommend'], ['We', 'enjoyed'], ['They', 'flew to'], ['She', 'visited'], ['He', 'stays in'],\n",
    "    ['I', 'discovered'], ['We', 'drive'], ['They', 'departed'], ['She', 'return'], ['He', 'be in'],\n",
    "    ['I', 'hiked in'], ['We', 'visit'], ['They', 'traveling'], ['She', 'was'], ['He', 'left']\n",
    "    ]\n",
    "    neutral_prefixes = [\n",
    "    ['I', 'have'], ['We', 'found'], ['They', 'like'], ['She', 'ate'], ['He', 'saw'],\n",
    "    ['I', 'own'], ['We', 'discovered'], ['They', 'use'], ['She', 'bought'], ['He', 'enjoys'],\n",
    "    ['I', 'sold'], ['We', 'lost'], ['They', 'have'], ['She', 'made'], ['He', 'found'],\n",
    "    ['I', 'dropped'], ['We', 'showed'], ['They', 'ate'], ['She', 'threw'], ['He', 'washed'],\n",
    "    ['I', 'bought'], ['We', 'borrowed'], ['They', 'cooked'], ['She', 'read'], ['He', 'cleaned'],\n",
    "    ['I', 'wrote'], ['We', 'gave'], ['They', 'painted'], ['She', 'played with'], ['He', 'picked up']\n",
    "    ]\n",
    "    person_names = [\n",
    "    'John', 'Alice', 'Michael', 'Emma', 'Daniel', 'Sophie', 'Lucas', 'Olivia', 'Ethan', 'Isabella',\n",
    "    'Mason', 'Ava', 'Logan', 'Mia', 'Liam', 'Gal', 'Noah', 'Ella', 'Aiden', 'Amelia',\n",
    "    'Elijah', 'Avery', 'James', 'Scarlett', 'Benjamin', 'Grace', 'Jacob', 'Chloe', 'Matthew', 'Evelyn'\n",
    "    ]\n",
    "    locations = [\n",
    "    'Paris', 'Berlin', 'New York', 'London', 'Tokyo', 'Rome', 'Sydney', 'Toronto', 'Moscow', 'Dubai',\n",
    "    'Singapore', 'Madrid', 'Istanbul', 'Yavne', 'Amsterdam', 'San Francisco', 'Acre', 'Bangkok', 'Budapest', 'Vienna',\n",
    "    'Mumbai', 'Shanghai', 'Cairo', 'Rio', 'Melbourne', 'Barcelona', 'Dublin', 'Seoul', 'Athens', 'Vancouver'\n",
    "    ]\n",
    "    neutral_words = [\n",
    "    'book', 'apple', 'chair', 'dog', 'car', 'table', 'phone', 'coffee', 'pen', 'glasses',\n",
    "    'shirt', 'shoes', 'bag', 'hat', 'watch', 'wallet', 'camera', 'computer', 'cake', 'bread'\n",
    "    ]\n",
    "    postfixes = [['last', 'week'], ['this', 'month'], ['next', 'year'], ['every', 'day'], ['on', 'Sunday']]\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for _ in range(n):\n",
    "        category = random.choice(['Person', 'Location', 'None'])\n",
    "        if category == 'Person':\n",
    "            prefixes = person_prefixes\n",
    "            target_words = person_names\n",
    "        elif category == 'Location':\n",
    "            prefixes = location_prefixes\n",
    "            target_words = locations\n",
    "        else:  # None\n",
    "            prefixes = neutral_prefixes\n",
    "            target_words = neutral_words\n",
    "\n",
    "        label = category\n",
    "\n",
    "        prefix = random.choice(prefixes)\n",
    "        target = random.choice(target_words)\n",
    "        postfix = random.choice(postfixes)\n",
    "\n",
    "        sentence = f\"{prefix[0]} {prefix[1]} {target} {postfix[0]} {postfix[1]}\"\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = generate_sentences(10000)\n",
    "\n",
    "# Vectorize the input (features and labels)\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(sentences)\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_vec = le.fit_transform(labels)\n",
    "\n",
    "# Define the Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index].toarray(), self.targets[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "# Split into training and testing\n",
    "split_ratio = 0.8\n",
    "split_idx = int(split_ratio * len(sentences))\n",
    "train_dataset = MyDataset(X_vec[:split_idx], Y_vec[:split_idx])\n",
    "test_dataset = MyDataset(X_vec[split_idx:], Y_vec[split_idx:])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = len(vectorizer.get_feature_names_out())\n",
    "hidden_dim = 64\n",
    "output_dim = len(le.classes_)\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Classifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the loss and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_dataloader):\n",
    "        inputs = inputs.squeeze(1).float()\n",
    "        targets = targets.long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs = inputs.squeeze(1).float()\n",
    "        targets = targets.long()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    # Prepare the input\n",
    "    X = vectorizer.transform([sentence])\n",
    "\n",
    "    # Convert the input to a tensor\n",
    "    X_tensor = torch.from_numpy(X.toarray()).float()\n",
    "\n",
    "    # Pass the input through the model\n",
    "    outputs = model(X_tensor)\n",
    "\n",
    "    # Use softmax to get the probabilities and take the argmax to get the predicted classes\n",
    "    _, predicted_classes = torch.max(nn.functional.softmax(outputs, dim=1), 1)\n",
    "\n",
    "    # Decode the predicted classes\n",
    "    predicted_tags = le.inverse_transform(predicted_classes.detach().numpy())\n",
    "\n",
    "    # Return the predicted tag for the sentence\n",
    "    return predicted_tags[0]\n",
    "\n",
    "# Use the function\n",
    "\n",
    "tag = predict(\"I meet iris next week\")\n",
    "print(f'The predicted tag for the sentence is: {tag}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
